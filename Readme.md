# ğŸ™ï¸ **Fake My Voice**
> **â€œClone any voice. Speak in anyoneâ€™s tone.â€**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Fake My Voice** is a deep learningâ€“based project focused on voice cloning and speaker-conditioned speech synthesis. The system generates realistic speech that sounds like a specific person using only a few seconds of their audio. 

By combining **Speaker Embeddings**, **Sequence-to-Sequence Synthesis**, and **Neural Vocoding**, we achieve high-quality, human-like output for multi-speaker Text-to-Speech (TTS).

---

## ğŸ“– **Table of Contents**
1. [ğŸŒŸ About the Project](#-about-the-project)
2. [ğŸ¯ Aim](#-aim)
3. [ğŸ—ï¸ Architecture](#-architecture)
4. [ğŸ› ï¸ Tech Stack](#-tech-stack)
5. [ğŸ“Š Dataset](#-dataset)
6. [ğŸ“ˆ Results & Demos](#-results--demos)
7. [ğŸ“ File Structure](#-file-structure)
8. [ğŸš§ Challenges Faced](#-challenges-faced)
9. [ğŸ‘¥ Contributors](#-contributors)

---

## ğŸŒŸ **About the Project**
- ğŸ§ **Voice Cloning:** Replicate a target speaker's voice with minimal reference audio.
- ğŸ¤– **Deep Learning Pipeline:** Utilizes state-of-the-art models like Tacotron2 and WaveGlow.
- ğŸ”Š **High Quality:** Combines speaker-conditioned synthesis with neural vocoding for natural results.

---

## ğŸ¯ **Aim**
- Build a multi-speaker system capable of replicating tone, accent, and style.
- Implement a unified pipeline consisting of **Tacotron2**, **GE2E Speaker Encoder**, and **WaveGlow**.
- Produce natural speech from text with minimal data per speaker.

---

## âš™ï¸ **System Overview**
1. **Extract:** Derive a speaker embedding from reference audio.
2. **Synthesize:** Use embedding + text to generate a Mel-spectrogram.
3. **Vocode:** Feed the Mel-spectrogram into a vocoder for waveform generation.
4. **Output:** Deliver speech mimicking the original speaker.

---

## ğŸ› ï¸ **Tech Stack**

| Category | Tools / Frameworks |
| :--- | :--- |
| **Language** | Python ğŸ |
| **Frameworks** | PyTorch, NumPy, Librosa |
| **Audio Tools** | Torchaudio, SoundFile, Matplotlib |
| **Models** | GE2E Encoder, Tacotron2, WaveGlow |
| **Visualization** | TensorBoard ğŸ“ˆ |

---

## ğŸ—ï¸ **Architecture**

![architecture](images/architecture02.png)

### 1. **Speaker Encoder (GE2E)** ğŸ§ 
- Extracts a fixed-dimensional embedding (**256-D**) capturing vocal identity.
- Uses **Generalized End-to-End (GE2E)** loss to cluster same-speaker embeddings closely and separate different speakers.
- **Input:** Short raw audio samples.
- **Output:** Speaker embedding vector.

### 2. **Speech Synthesizer (Tacotron2)** ğŸ¹
- Converts text to Mel-spectrograms conditioned on the speaker embedding.
- **Encoder:** Processes phoneme/character embeddings with convolutional and recurrent layers.
- **Location-Sensitive Attention:** Ensures smooth, monotonic progression and prevents word-skipping.
- **Decoder:** Autoregressively generates frames using $L1$ loss.

### 3. **Vocoder (WaveGlow)** ğŸ—£ï¸
- Converts high-resolution Mel-spectrograms into time-domain waveforms.
- **Input:** Mel-spectrogram from Tacotron2.
- **Output:** Final waveform audio in the cloned voice.

---

## ğŸ“Š **Dataset**

| Dataset | Purpose | Details |
| :--- | :--- | :--- |
| **LJSpeech** | Tacotron2 | ~13k samples, Single Speaker |
| **VoxCeleb** | Speaker Embeddings | ~5k samples, 40 English Speakers |
| **VCTK Corpus** | Merged Model | 44 Hours, 109 English Speakers |

> **Note:** Audio was preprocessed by resampling from 48 kHz to **22.05 kHz**, normalized, and truncated to $\le 4$ seconds.

---

## ğŸ“ˆ **Results & Demos**

### **Tacotron2 Performance**
Achieved smooth Mel-spectrogram prediction after ~60 epochs.

**Text:** *"Printing, in the only sense with which we are at present concerned"*

| Expected Mel | Predicted Mel | Predicted Frames |
| :---: | :---: | :---: |
| ![Expected mel](images/target_mel.png) | ![mel predicted](images/mel_64.png) | ![output frames](images/output_frames.png) |

### **Speaker Embeddings**
Achieved stable separation across 40+ speakers. t-SNE plots show distinct identity clustering.
![speaker identites](images/embeddings.jpg)

### **Audio Output**
[Download/Listen to Audio Sample](images/model_output_fmv.wav)

> *"Use this model to clone the voice of any user"*

### **Losses & Metrics**
- **Mel Prediction:** Mean Squared Error (MSE) Loss.
- **Stop Token:** Binary Cross-Entropy (BCE) Loss.
![loss graph plot](images/loss.png)

---

## ğŸ“ **File Structure**

```bash
Fake_My_Voice/
â”œâ”€â”€ Multi-Speaker-TTS/      # Multi-speaker synthesis logic
â”œâ”€â”€ SingleSpeaker_TTS/      # Baseline TTS code
â”œâ”€â”€ Speaker_Embeddings/     # GE2E extraction scripts
â”œâ”€â”€ datasets.txt            # Training data references
â”œâ”€â”€ requirements.txt        # Project dependencies
â””â”€â”€ README.md               # Documentation
```
---

## ğŸš§ **Challenges Faced**

Working with multi-stage neural TTS pipelines presented several technical hurdles:

* âš–ï¸ **Length Mismatch:** Resolving discrepancies between Mel-spectrogram frames and waveform samples during loss calculation.
* ğŸ§ **Sampling Rate Consistency:** Standardizing audio from various sources (48 kHz to 22.05 kHz) to ensure uniform feature extraction.
* ğŸ”‹ **GPU Optimization:** Managing the high VRAM footprint of Tacotron2 and WaveGlow, especially during concurrent training.
* ğŸ”‡ **Alignment Stability:** Tackling "silent outputs" or word skipping by fine-tuning the location-sensitive attention mechanism.
* ğŸ“ˆ **Embedding Sensitivity:** Ensuring training convergence by prioritizing high-quality, distinct speaker embeddings from the GE2E encoder.

---

## ğŸ‘¥ **Contributors**

We are a team of passionate developers exploring the intersection of Speech Synthesis and Deep Learning.

* ğŸ’» **Aryan Doshi**
* ğŸ’» **Dhiraj Shirse**
* ğŸ’» **Nihira Neralwar**

---

## ğŸ“ **Mentors**

A special thanks to our mentors for their technical guidance and support throughout the project:

* **Kevin Shah**
* **Prasanna Kasar**
* **Yash Ogale**

---

## ğŸ“œ **Acknowledgements & References**

### **Community & Organizations**
- **Community of Coders (CoC)** and **Project X VJTI** for providing the platform and resources to build this project.

### **Research Papers**
- ğŸ“„ [Tacotron2: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/pdf/1712.05884)
- ğŸ“„ [GE2E: Generalized End-to-End Loss for Speaker Verification](https://arxiv.org/pdf/1710.10467)
- ğŸ“„ [WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/pdf/1811.00002)

### **Datasets & Implementations**
- ğŸ“‚ [LJSpeech Dataset](https://www.kaggle.com/datasets/dromosys/ljspeech)
- ğŸ“‚ [VoxCeleb Dataset](https://www.kaggle.com/datasets/bachng/voxceleb)
- ğŸ“‚ [VCTK Corpus Dataset](https://www.kaggle.com/datasets/pratt3000/vctk-corpus)
- ğŸ› ï¸ [NVIDIA Tacotron2 + WaveGlow PyTorch Implementation](https://github.com/NVIDIA/tacotron2)

---
<p align="center">Made with â¤ï¸ by the Fake My Voice Team</p>
